M: I want to start with you Amy because I know you, obviously at Shell, have had AI as part of the working process, creating efficiencies, a number of different use cases. You're using it to kind of transcend as well, at least do the transformation away from some of the carbon parts of the business towards that green evolution and you see that as consequential. How do you look, how do you assess what the needs are and then isolate and focus on the potential solutions before we get onto the question of how you incorporate that? How do you, how do you, how do you zero in on where the needs are across the business?
R: I think it's like with any improvement initiative, you're going to have a lot of different ways you look for ideas and you can do them top down, you can do benchmarking, you can say where have we got high areas of cost and what's your intuition, what did you learn in your last business? And you can do it bottom up as well, where you ask the machine operator, how is this process stupid? How could you do better? Also you look for any kind of very manual process or something which is very tricky or often goes wrong. I think the trouble is there's opportunity everywhere. The only difference is how much value it is. So once you've got your kind of long list of ideas or areas you think you could look at, it's then a question of your standard two by two matrix of feasibility, how easy is it versus value. And that's the piece we use to to from the beginning say, do we think this is worth looking at? And then as we go through, as we learn more about the problem, you often find actually it's harder than you thought, or it's easier than you thought, or the value isn't there for some reason. And then you adjust, but basically it's the usual, I don't actually think it's that different from any other lever you can pull in business in terms of process redesign and the rest of it. Because it's also it's not just about efficiency, often it's a quality thing as well. If you want to improve customer service, you don't just want to talk to more customers in an hour and treat more of them. You also want to provide a better service, make sure you answer their query quickly. But fairly standard.
M: Is it is it a harder sell to the board because of the some of the unknowns around at least the new generative parts of AI? The generative AI. Or do you get it across the board as easily as you would any other potential?
R: It's an interesting one, because I think with AI and particularly generative AI, you have your usual problem with AI, which is on the one hand people think it's magic. Magic is an easy sell. On the other hand, you have to deliver to magic and that's not easy. So it's always this mix of you have to do a certain amount of marketing, you have to tell people what we can do genuinely. If you haven't seen it before, this is the amazing stuff we can do. But also set expectations that this isn't magic, it's hard work, it often fails. It requires just as much process change, change management, impact from the business as anything else. And that's a very balanced message. But I've certainly found with our leadership, there's a lot of receptivity and it's there's a lot of excitement at the moment.
M: Tony, let's bring you in at this point. Cloud services, the cloud essential to what you what you do. And Temenos recently launching an AI powered product as well for transactions. So to make it easier for your clients, financial services and banking clients to be able to kind of pull out at least monitor and give detailed information on on transactions for for their clients and their customers as well. How how do you identify the need? How have you been thinking about this and some of the potential pitfalls and opportunities around how to implement?
R: Yeah, I think to to build on what Amy was saying, it's it's a lever, right? And we were joking just before in the over coffee that the the answer is generative AI. Now, what's your problem? Um and that's a pitfall. Um so we're at Temenos, we're a big fan of appropriate use of technology. Um and that can be the pragmatic stuff. Um a lot of it, again, building what Amy was saying, is around not only the AI elements, the culture and the organization as well. You you are changing people's jobs, that's part of that's part of what we're doing and hopefully for the better. Um and so when you look at things like, you know, transaction classification is not new. We've been doing it for years and years based off, you know, originally rules, then traditional AI, if I can say that at this point, um and then moving into explainable AI and now generative AI. So it's layering on these tools and techniques, coupling it with the the culture change and the the underlying organization change that you need as well.
M: Does that ring true with you, Alex, at Ocado?
R: Absolutely. At Ocado, we really first look at the business end to end, understand the operating costs uh of our retail partners. So Ocado, we are no longer Ocado retail. Uh we own 50% of Ocado retail along with M&S. Uh Ocado's business today is in providing an end-to-end e-com grocery logistics platform for international retailers all over, so Kroger, Coles, Aeon, uh to name just a few. Uh and teams that work for me and across Ocado technology, we focus really on understanding uh the customers' the customer's business model. And so, you know, last mile logistics is a very significant uh proportion of sales in terms of the cost, uh as

--- NEXT SEGMENT ---

R: the customer's business model. And so, you know, last-mile logistics is a very significant proportion of sales in terms of the cost, as is operating the warehousing, the logistics. And so that's where we really focus an awful lot of our innovation is in these two areas of very high cost for retailers to operate a logistics platform. But we need to have machine learning and AI pervasive through the end-to-end capability in order to be able to realize those sort of incremental gains that really lead to fundamentally either having a profitable business or not because of, you know, how thin the margins are in grocery. And so, you know, interaction from the webshop through to the warehouse, supply chain, routing systems, all the way to last-mile grocery logistics, the delivery, have to be very tightly coupled together. And insights from one have to be fed through. And so, you know, customer shopping habits influence how we design warehouses. And then all the way through into using AI to do things like picking and packing, which is what my team does at the moment. All the retailers around the world range different items. In the UK, they range 70,000 different types of items. And so the diversity and complexity is huge. There's no way you can build systems using traditional technologies and heuristics. One really has to embrace systems that learn from data, such as AI and doing that by really focusing the investment where it generates that return for.
M: How do you decide whether to build in-house or to buy in or pull in third parties?
R: So Ocado in effect is monetizing its intellectual property. And so we have to be very careful about how we use third-party technology in the way that it's licensed, but we leverage an awful lot of common AI technology and tooling such as TensorFlow. A few years ago, we were working on a project to predict the failures in some of our, of actual automation, physical automation. I came back from Aerospace a few years ago. And so we leveraged something called Google WaveNet, which was a machine learning capability developed by Google for time domain signals. It was traditionally developed for speech recognition, but you are able to adapt this technology for learning any time domain signal and then, you know, if you are in the process of physical automation, motors, sensors, these all generate time domain data streams, and so we leverage things like WaveNet. And then we build our own, the latest control system that we have, leverages a different type of generative AI, not language models, but actually it's called behavior cloning, where we use the cognitive ability of humans to effectively remote control the robots to start with. We then use that data to train the AI and it is end-to-end, right? So video comes in, robot control comes back out, and the ability of these systems to generate, to generalize across problems they haven't seen is phenomenal. I mean, it's just it truly blows my mind in terms of the capability.
M: So there's the human training element and then the human step back, steps back and they're able to continue these functions.
R: But AI isn't perfect. AI goes wrong, and so how we leverage humans and AI working collaboratively together. If the robot for whatever reason is unable to complete a pick, maybe it drops an item, maybe packaging has changed, packaging has been damaged or something, then we're able to use remote operators to effectively collaborate with the AI to help complete the task. So it performs operational recovery in order to maintain that kind of very reliable service that we need 24/7 picking millions of items a day. And then we can use the data from a failed pick that has been recovered by this remote pilot to go and improve the AI over time. And so having a strategy where you build technology that for us this remote operation stuff, it helps us build the ML at the beginning. It helps us operate it in terms of the the real-world operational challenges and then that it also helps us kind of build further improvements and so you're kind of generating inertia in this flywheel that gets faster and faster and faster.
M: There's a combination then of in-house solutions and third-party. When when it comes to you, what about it Shell Amy? How do you think about whether to build it out in-house? Do you have the resources? Do you have the team and you've got your cash rich company? Do you do it in-house or do you you pull in third parties?
R: I think similar to Alex, I don't think anyone could say they're building everything in-house nowadays because your component parts, the component parts you get from outside are just so good now, you'd be crazy to try and do it on an island. But I mean we we always think in terms of is it our core IP? What are we? We are we produce energy, we process it, we distribute it and we trade it. That's core to what we do. And if it's something outside of that, which isn't completely easy or obvious or anything like that, then we should use market standard. And if it's something within that, then we might want to look carefully at whether it's our core IP and therefore we want to keep it. I think build versus buy is a bit simple though because there's plenty of things like partner or build on something that's actually third party. And obviously we we use cloud platforms like everyone else. We're not going to try and use our component parts. So actually it's it's always a patchwork of taking what you have from the market, building on.

--- NEXT SEGMENT ---

R: cloud platforms, like everyone else. We're not going to try and use our component parts. So actually, it's, it's always a patchwork of taking what you have from the market, building on what you need internally, and just being really clear about how best to use your time and resources.
M: How do you find, are there frictions when it comes, you think of an oil company or an energy company like Shell, and you think of engineers out there on rigs. You have those engineers, but you also have your software engineers building out some of these AI solutions. How do you meld the two? Is there a friction there? How do you get past that?
R: That's a different type of friction. I mean, I don't see that so much with the frontline necessarily. That's always a change management question that you're, with any population, whatever it is, whether it's call center workers or people on an oil rig. It's always going to be some people think, brilliant, let's try something new, let's see how we can do this differently. Some people are like, I don't know, it's worked so fine so far for 25 years, why change it? And other people are kind of could go either way. So it's always an effort to, to really do the change management, make sure everyone's bought in, make sure it's explained properly, make sure it is to everyone's benefit as well. Of course, people don't want to adopt things which they're not sure about. But I mean, that's a very different type of tension, I think. And that's one which again, is no different from any other process change, any other change to a business that you'll have.
M: Tony, is it, is it, is it easier for you because you're a pure software play? Maybe you don't have those same, I mean, you're smiling at this, so maybe those tensions are there. How do you, how do you, once you've decided, you've selected what the problem is, you've chosen whether you're going to do it in-house or pull in a third party, make an acquisition, you've found out what the, what the problem you're trying to tackle is, and then you have a team tackle it with that particular solution. How do you embed that then into the business and mitigate the downside risks?
R: So I think you've been reading some of our blog posts because the keyword there is embed, right? So, so yes, much, and again, to, to not necessarily just say yes, as well. But um, yeah, we take a mixed approach. We're not going to build out the nuts and bolts. There's a great deal of open source out there that we want to use and again, partnering is a, is another angle. But then, um, you know, pulling it all together is where it starts to, the magic happens, as it were. So in terms of what we try to do, and we try to encourage our clients, the banks to do as well, is not sort of put AI over there in a corner. We want all of our product managers, all of our product teams, all of our engineering teams to be able to use this platform capability, which, you know, we may well change over time, and that's fine. That's why we put a platform over the top of it, so that they can then build our core IP, banking IP, over the top of those models, over the top of those capabilities, regardless of what the underlying AI tech is. So it's about exactly that. Embedding it into the teams, embedding it into the products. That's how we've seen a lot of our customers be successful and that's how we're rolling out our AI-powered products as well.
M: A lot of this innovation is obviously happening far ahead and moving far more quickly than the regulators are, and you work obviously in a very regulated space. So how, how do you, how do you keep ahead of your competitors whilst also assuring, ensuring that you don't fall foul of the regulators?
R: So one of the things that we talk about a lot is this concept of responsible AI. And you know, previously we talked about um explainable AI and ethical AI. So we kind of roll it all together into this concept of responsible AI. And again, we were talking at the break about how we can see legislation coming down the track. We know it's coming, but moreover, um, it's a duty of care, if you will, so that if somebody um has a loan application rejected, they need to understand why. You can't be using black box models that just, you know, computer says no, you need to understand why. You need to be able to challenge that, and you need to provide evidence that if it is behavior-based, so again at the break we were talking about the idea that perhaps as a, as an SME, as a leader in a small company, you may not get a loan because your cholesterol is high. But unless you know that's what the problem is, then you don't have a way of fixing it, and you don't have a way of challenging it. So, yes, there's all this regulation coming down the track, keeping an eye on it, looking to the wider industry, obviously, but also a lot of that regulation, hopefully, maybe this is a little um a little optimistic, is there to protect the consumers and ourselves. So really, you know, come back to do what's, what's good, what's correct, um is, is there, the North Star if you will.
M: Okay, so the importance of explainable models versus maybe the black box. So maybe a bit of a critique of of ChatGPT and Open AI on that level. There's a question actually from the audience around what your use is of of Open AI. So I'll get to that very shortly. But Alex, on on the question of of regulation, on the question of data. Obviously, your clients are customer-facing. I may not want your clients to know

--- NEXT SEGMENT ---

M: get to that very shortly.
M: Uh but Alex, on the question of regulation, on the question of data. Obviously, your clients are customer-facing. I may not want your clients to know that on a Friday I buy 10 tubs of ice cream and a bottle of whiskey.
M: So how do you think about data and abuses of data and how to ring fence that so that you don't lose that client trust?
R: Uh, so again, we have a responsible AI policy that is being heavily influenced.
R: client to be tracked for data and we ensure lack of biases.
R: Uh fundamentally, in terms of your question, how do we ensure that kind of confidence in our retail partners?
R: We don't track individual people, basically, right? They they hold all of that data in their own systems and what we get is effectively tokenized and anonymized data.
R: So that we can use that to improve the systems without having any access to any individual customer.
M: So do you use on this question that's come through from one of our viewers or someone in the room, Open AI, do you use it in your systems?
R: Uh not within our systems. At the moment, sort of the free to use Chat GPT, everything that you put into it is public domain, and so we are being very careful about how we do use it.
R: However, our teams of engineers do leverage it as part of supporting them doing their work.
R: Uh I have a team of engineers 3D printing a robot arm.
R: And actually, we were exploring different permutations of which print technology in order to drive cost and the right kind of mechanical performance, and so we leveraged Chat GPT to explore different permutations with different types of print technology.
R: by posing it a series of questions. It came back with the answers in a split second.
R: We validated a number of them for accuracy, they were 100% accurate.
R: and then we just believe the rest, I'm afraid.
R: Uh, so as a support tool, if you understand how to write the right query,
R: Yeah.
R: Uh, it is incredibly powerful.
M: Next time bring the arm in with you.
R: I will happily do that.
M: Okay, so use Chat GPT4, the hallucinations you stress test for some of those. Amy, you use, you've allowed Chat GPT4 to be used at Shell.
R: Not me personally, but yeah.
M: Okay. Some some institutions, some some banks, for example.
R: No, we certainly are using um, well, we haven't actually blocked um public Chat GPT yet.
R: It's we've trained people, but we haven't blocked that.
R: But more importantly, we are using the APIs by Microsoft on Open AI, on Chat GPT, um to create services in Shell.
R: And to run a series, we were a whole program using that and other Gen of AI models, both in vision and text.
R: to explore what we can do around the business to improve efficiency for sure, but also look at quality and other impacts.
R: So we do have a large generative AI program now.
M: Okay.
M: Uh another question coming through from the audience, where do you see the trend of automation going, especially when we're influenced by chip shortages and AI enthusiasm?
M: Tony, let's put that question to you. Nvidia was very much in the headline the last few weeks, some of their most high-end chips cost about 20, 30,000 US dollars. We have a shortage of them here in the UK. What what is your take on that question?
R: So, to an extent, the answer's cloud.
R: Now, at the risk of coming back to the nuances, the answer's cloud now what's your problem?
R: Um, you know, the ability to put pragmatic use of technology, put in the right workloads on the most efficient chips.
R: Cloud gives you that ability, most banks typically for historic reasons will have an amount of tin that they can reuse.
R: It's unlikely they've got vast amounts of GPUs or AI-friendly kit lying around.
R: So um, and you know, in 2023, do you want to be building that?
R: Now, there's um my friends at IBM, uh we've had a few discussions on this and the idea of a hybrid cloud where some of our customers, some of our banks need to keep this data on-prem, or want to, for whatever reasons, um then, you know, there's a different set of chip technology that they can bring to bear there.
R: So cloud but hybrid um so yeah, a combination of those two, again, appropriate technology for the right for the right use case.
M: Alex, as you project out five years, where do you see automation taking us?
M: as we weigh up these two, it seems conflicting pressures, the shortage of chips and this hype and enthusiasm around AI.
R: Uh, in the space that we operate, I mean, I see it becoming ever more efficient our operation.
R: Uh for the retailers that we support fundamentally and without going to a different challenging question about AI and jobs. Uh the main uh direct costs that our retailers faced is uh, you know, fairly low skill labor roles, whether they are picking packing in a warehouse or doing other types of handling.
M: They will have jobs, aren't they?
R: I I'm afraid, I kind of think that at some point in the event horizon they are, uh moving in uncertain environments, if you're on a road, is much more complicated than handling things in a warehouse, which is where we are focusing at the moment.
R: Uh when it comes to the challenge and constraints of chip availability. Actually, Acardo years ago, we uh encountered our threshold of how much spend we are willing to make in terms of cost for cloud costs and so.

--- NEXT SEGMENT ---

R: Actually, at Ocado years ago, we encountered our threshold of how much spend we are willing to make in terms of cost for cloud costs.
R: And so today, we are very conscious when we build new systems about the amount of logging that we're creating, the amount of data that we're storing.
R: Our ability to generate data versus the utility we get from that data, it far exceeds the potential benefits, and so actually having a very strong focus on how you maximize your utility from data.
R: Which therefore ultimately ends up meaning we have to be quite efficient with the hardware that we're using.
R: When we deploy robots, the robots need to be for a certain price point, and so I'm afraid we can't deploy them with just like tens of GPUs.
R: And so actually we have to find ways of deploying very advanced physical automation systems and also software-based automation systems that don't just consume all the compute that you can throw at it.
R: So actually, being very efficient with data and then, which means being efficient with compute at the same time is key.
M: Amy, how much free rein do you have to spend on compute?
M: Are you concerned that the lack of compute, the shortage of chips, the cost could could hold back some of these innovations?
R: Yes. I think the supply chains are currently very fragile, and there are efforts being made to make them more robust with government support in various countries.
R: That will take some time to flow through, so I think the combination of the absolute hype and the fragile supply chain could lead to a bit of a crunch.
R: Whether that's actually going to impact everyday operations, I'm not sure, because these things take time to develop and really put into practice anyway, but that definitely is a concern.
M: There's another question coming in about quantum computing and the impact on the pace.
M: In terms of how quantum computing might may speed up the implementation of AI.
M: Anyone kind of jumping out with a view, a strong view on that?
R: I think we'll take some time.
M: It'll take some time. Still a ways off.
R: We have researchers dabbling occasionally. There's a problem called the Knapsack problem, which is a very classic optimization problem. It looks not that dissimilar to a cubing problem that we have where we theoretically tessellate all of the potential groceries into our shopping baskets.
R: As you are clicking add to basket, we are doing that tessellation live so that we can maximize the efficiency of the traveling salesman problem, that is the last mile grocery logistics.
R: And at the point that we've tried to formulate this problem into a quantum problem, the number of qubits we need is in the millions, and at the moment D-wave machines are in the thousands, and maybe there's some governments out there with machines which are far more powerful.
R: But for me, when we've dabbled in it, it looks very distant.
M: Not yet, for sure.
M: Not yet.
M: We're talking time frames of years, decades?
R: I'm happy to, I mean, depending on who you talk to, quantum computing is tomorrow or 300 years away.
M: Right, so somewhere between.
M: Somewhere between that. Alright, fantastic.
M: Well thank you very much.